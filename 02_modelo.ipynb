{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "365e891c",
   "metadata": {},
   "source": [
    "# Detección de anomalías basada en KMeans + distancia\n",
    "\n",
    "Este análisis utiliza los datos de ayudas PAC 2024, realiza ingeniería de variables, entrena KMeans, calcula una puntuación de anomalía por distancia al cluster y por último genera un dataset enriquecido listo para publicarse en un espacio de datos.\n",
    "\n",
    "- *FEADA* -> Fondo Europeo Agrícola de Garantía\n",
    "- *FEADER* -> Fondo Europeo Agrícola de Desarrollo Rural\n",
    "- *IMPORTECOFIN* -> Cofinanciación nacional/regional\n",
    "- *FEADER_COFIN* -> Parte cofinanciada dentro del FEADER\n",
    "- *IMPORTE_EUROS* -> Importe total findal de la ayuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c796b025",
   "metadata": {},
   "source": [
    "## Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89268dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/09/08 22:58:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, functions as F, types as T\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler, CountVectorizer, RegexTokenizer\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.linalg import DenseVector, VectorUDT\n",
    "from pyspark import StorageLevel\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Anomalias-KMeans\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3168d739",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"data/espacio_datos/pac_2024_clean\"\n",
    "output_path = \"data/espacio_datos/pac_2024_anomalias_kmeans/\"\n",
    "\n",
    "# Definir rango para buscar k\n",
    "k_range = list(range(3, 5)) \n",
    "# Definir percentil para marcar anomalías (98% = top 2% más alejados)\n",
    "pctil_of_anomaly = 0.98  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5ed849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer datos\n",
    "df = spark.read.parquet(input_path)\n",
    "df.show(20,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b926fc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalización de nulos numéricos\n",
    "num_cols = [\"FEAGA\", \"FEADER\", \"IMPORTECOFIN\", \"FEADER_COFIN\", \"IMPORTE_EUROS\"]\n",
    "fill_num = {c: 0.0 for c in num_cols}\n",
    "df = df.fillna(fill_num)\n",
    "\n",
    "# Aplicar log transform para reducir asimetría y estabilizar varianza\n",
    "df = (df.withColumn(\"IMPORTE_ABS\", F.abs(F.col(\"IMPORTE_EUROS\")))\n",
    "       .withColumn(\"IMPORTE_SIGN\", F.when(F.col(\"IMPORTE_EUROS\") < 0, -1.0).otherwise(1.0))\n",
    "       .withColumn(\"IMPORTE_LOG\", F.col(\"IMPORTE_SIGN\") * F.log1p(F.col(\"IMPORTE_ABS\")))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ebbac5",
   "metadata": {},
   "source": [
    "## Feature engeeniring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45968776",
   "metadata": {},
   "source": [
    "### Crear ratios de composición"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ebf293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar el total absoluto para que las proporciones sean interpretables incluso en recuperaciones\n",
    "safe_den = F.when(F.col(\"IMPORTE_ABS\") > 0, F.col(\"IMPORTE_ABS\")).otherwise(F.lit(None))\n",
    "\n",
    "\n",
    "df = (df.withColumn(\"FEAGA_ratio\", F.col(\"FEAGA\")/safe_den)\n",
    "       .withColumn(\"FEADER_ratio\", F.col(\"FEADER\")/safe_den)\n",
    "       .withColumn(\"COFIN_ratio\", F.col(\"IMPORTECOFIN\")/safe_den))\n",
    "\n",
    "# Reemplazo de posibles nulls de ratio por 0.0\n",
    "for c in [\"FEAGA_ratio\",\"FEADER_ratio\",\"COFIN_ratio\"]:\n",
    "    df = df.withColumn(c, F.coalesce(F.col(c), F.lit(0.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abde0ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizar etiqueta con multipes posibles (OE4|OE5|OE6)\n",
    "splitter = RegexTokenizer(\n",
    "    inputCol=\"OBJETIVO_ESP\",\n",
    "    outputCol=\"obj_esp_tokens\",\n",
    "    pattern=\"\\\\|\",\n",
    "    gaps=True\n",
    ")\n",
    "cv = CountVectorizer(inputCol=\"obj_esp_tokens\", outputCol=\"obj_esp_vec\", binary=True, minDF=2)\n",
    "\n",
    "# One hot encode de variables categóricas\n",
    "cat_cols = [\"PROVINCIA_SAFE\", \"MEDIDA\"]\n",
    "indexers = [StringIndexer(inputCol=c, outputCol=f\"{c}_idx\", handleInvalid=\"keep\") for c in cat_cols]\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[f\"{c}_idx\" for c in cat_cols],\n",
    "    outputCols=[f\"{c}_oh\" for c in cat_cols],\n",
    "    handleInvalid=\"keep\" \n",
    ")\n",
    "\n",
    "# Variables muméricas\n",
    "numeric_cols = [\"IMPORTE_ABS\", \"IMPORTE_LOG\", \"IMPORTE_SIGN\",\"FEAGA_ratio\", \"FEADER_ratio\", \"COFIN_ratio\",\"IS_RECUP_ANY\"\n",
    "]\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"obj_esp_vec\"] + [f\"{c}_oh\" for c in cat_cols] + numeric_cols,\n",
    "    outputCol=\"features_raw\",\n",
    "    handleInvalid=\"keep\")\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\", withStd=True, withMean=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ef63ee",
   "metadata": {},
   "source": [
    "## Spark Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7645a402",
   "metadata": {},
   "source": [
    "### Seleccionar K por silhouette "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90eeb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline reutilizable (sin KMeans)\n",
    "pre_kmeans_stages = [splitter, cv] + indexers + [encoder, assembler, scaler]\n",
    "pre_pipeline = Pipeline(stages=pre_kmeans_stages)\n",
    "# Modelo\n",
    "pre_model = pre_pipeline.fit(df)\n",
    "df_prepared = pre_model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec731c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar K en el 20%\n",
    "df_eval = df_prepared.select(\"features\").sample(False, 0.2, seed=42)\n",
    "df_eval = df_eval.repartition(200).persist(StorageLevel.MEMORY_AND_DISK)\n",
    "df_eval.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93749af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar mejor k/score\n",
    "best_k = None\n",
    "best_score = float(\"-inf\")\n",
    "metrics = []\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(featuresCol=\"features\", predictionCol=\"cluster\", k=k, seed=17)\n",
    "    model = kmeans.fit(df_eval)\n",
    "    pred = model.transform(df_eval)\n",
    "    evaluator = ClusteringEvaluator(featuresCol=\"features\", predictionCol=\"cluster\", metricName=\"silhouette\")\n",
    "    score = evaluator.evaluate(pred)\n",
    "    metrics.append((k, score))\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_k = k\n",
    "\n",
    "print(f\"Mejor k por silhouette: {best_k} (score={best_score:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77aaa79",
   "metadata": {},
   "source": [
    "### Entrenar modelo con best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a3c37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_final = KMeans(featuresCol=\"features\", predictionCol=\"cluster\", k=best_k, seed=42)\n",
    "full_pipeline = Pipeline(stages=pre_kmeans_stages + [kmeans_final])\n",
    "full_model = full_pipeline.fit(df)\n",
    "pred = full_model.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50532e6",
   "metadata": {},
   "source": [
    "## Añadir puntuación de anomalía\n",
    "\n",
    "Se calcula de acuerdo a la distancia euclídea al centroide asignado. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2cc9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener centroides\n",
    "kmeans_model = full_model.stages[-1]\n",
    "centers = kmeans_model.clusterCenters()\n",
    "\n",
    "# UDF para distancia euclídea entre vector denso y centroide\n",
    "def euclidean_distance(v, center):\n",
    "    if v is None:\n",
    "        return None\n",
    "    if not isinstance(v, DenseVector):\n",
    "        v = DenseVector(v.toArray())\n",
    "    return float(sum((v[i] - center[i])**2 for i in range(len(center))) ** 0.5)\n",
    "\n",
    "dist_udf = F.udf(lambda v, cid: euclidean_distance(v, centers[cid]), T.DoubleType())\n",
    "\n",
    "pred = pred.withColumn(\"dist_to_center\", dist_udf(F.col(\"features\"), F.col(\"cluster\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648f61bd",
   "metadata": {},
   "source": [
    "### Z-score de distancia dentro de cada cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e9723b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_stats = (pred.groupBy(\"cluster\")\n",
    "                     .agg(F.mean(\"dist_to_center\").alias(\"mu\"), \n",
    "                          F.stddev_pop(\"dist_to_center\").alias(\"sigma\")))\n",
    "\n",
    "pred = (pred.join(cluster_stats, on=\"cluster\", how=\"left\")\n",
    "            .withColumn(\"anomaly_score\", (F.col(\"dist_to_center\") - F.col(\"mu\")) / F.col(\"sigma\")))\n",
    "\n",
    "# Percentil global de anomaly_score para umbral\n",
    "q = pred.approxQuantile(\"anomaly_score\", [pctil_of_anomaly], 1e-3)[0]\n",
    "\n",
    "pred = pred.withColumn(\"ANOMALIA\", (F.col(\"anomaly_score\") >= F.lit(q)).cast(\"int\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6a6c2c",
   "metadata": {},
   "source": [
    "## Guardar resultados en parquet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819cda6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Seleccionar columnas para outpur\n",
    "out_cols = [\n",
    "\"BENEFICIARIO\", \"PROVINCIA_SAFE\", \"MUNICIPIO_COD\", \"MUNICIPIO_NOMBRE\", \"MEDIDA\", \"OBJETIVO_ESP\",\n",
    "\"FEAGA\", \"FEADER\", \"IMPORTECOFIN\", \"FEADER_COFIN\", \"IMPORTE_EUROS\",\n",
    "\"IMPORTE_ABS\", \"IMPORTE_LOG\", \"IMPORTE_SIGN\",\n",
    "\"FEAGA_ratio\", \"FEADER_ratio\", \"COFIN_ratio\",\n",
    "\"IS_RECUP_ANY\", \"cluster\", \"dist_to_center\", \"anomaly_score\", \"ANOMALIA\"\n",
    "]\n",
    "\n",
    "final_df = pred.select(*out_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a75222",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    final_df\n",
    "    .write.mode(\"overwrite\")\n",
    "    .coalesce(1)\n",
    "    .parquet(output_path)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-spark-use-case (3.11.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
